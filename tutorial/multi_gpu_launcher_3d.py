import os
import itertools
import subprocess
import time
from multiprocessing import Process, Queue

XDEBench_DIR = f'../XDEBench'

# å¯ç”¨ GPU åˆ—è¡¨
gpus = [4]  # æ ¹æ®å®é™…æœºå™¨æƒ…å†µä¿®æ”¹

# =========================================
# =========================================
# model_list = ['FNO3d']            # gpu1 - 1.5h
# batch_size_list = [1, 2]             # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [64]             # å¯è°ƒ
# n_layers_list = [0]               # æ— ç”¨å‚æ•°
# modes_list = [[16, 16, 16]]       # ç¡®è®¤ modes <= img_size // 2
# n_heads_list = [0]                # æ— ç”¨å‚æ•°
# lr_list = [0.005, 0.001]          # å¯è°ƒ

# model_list = ['CROP3d']             # gpu2 - 2.5h
# batch_size_list = [1]               # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [64]                   # å¯è°ƒ
# n_layers_list = [0]                 # æ— ç”¨å‚æ•°
# modes_list = [[16, 16, 16]]         # ç¡®è®¤ modes <= img_size // 2
# n_heads_list = [0]                  # æ— ç”¨å‚æ•°
# lr_list = [0.001]             # å¯è°ƒ

# model_list = ['CNext3d']          # gpu3 - 6.5h
# batch_size_list = [1]             # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [48]                 # ç¡®è®¤ width è‡³å°‘èƒ½è¢«2æ•´é™¤ n_layers+1 æ¬¡
# n_layers_list = [3]            # ç¡®è®¤ img_size è‡³å°‘èƒ½è¢«2æ•´é™¤ n_layers æ¬¡
# modes_list = [[0, 0, 0]]          # æ— ç”¨å‚æ•°
# n_heads_list = [0]                # æ— ç”¨å‚æ•°
# lr_list = [0.01]           # å¯è°ƒ

model_list = ['FFNO3d']             # gpu4 - 4h
batch_size_list = [1]               # ç¡®è®¤ batch_size <= num_train_traj
width_list = [64]                   # å¯è°ƒ
n_layers_list = [4]                 # å¯è°ƒ
modes_list = [[32, 32, 32]]         # å¯è°ƒ
n_heads_list = [0]                  # æ— ç”¨å‚æ•°
lr_list = [0.001]                   # å¯è°ƒ

# model_list = ['UNO3d']              # gpu5 - 8h
# batch_size_list = [1]               # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [64]                   # å¯è°ƒ
# n_layers_list = [0]                 # æ— ç”¨å‚æ•°
# modes_list = [[16, 16, 16]]         # å¯è°ƒ
# n_heads_list = [0]                  # æ— ç”¨å‚æ•°
# lr_list = [0.001]                   # å¯è°ƒ

# model_list = ['DPOT3d']             # gpu6 - 2.5h
# batch_size_list = [1]               # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [1024]                  # å¯è°ƒ
# n_layers_list = [6]                 # å¯è°ƒ
# modes_list = [[32, 32, 32]]         # ç¡®è®¤ modes <= img_size // 2
# n_heads_list = [0]                  # æ— ç”¨å‚æ•°
# lr_list = [0.001]                   # å¯è°ƒ

# model_list = ['Transolver3d']       # gpu7 - 8h
# batch_size_list = [1]               # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [96]                  # å¯è°ƒ
# n_layers_list = [4]                 # å¯è°ƒ 
# modes_list = [[0, 0, 0]]            # æ— ç”¨å‚æ•°
# n_heads_list = [4]                  # å¯è°ƒ 
# lr_list = [0.005]                   # å¯è°ƒ

# model_list = ['PhysicsTransformer']       # gpu7 - 8h
# batch_size_list = [11]               # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [200]                  # å¯è°ƒ
# n_layers_list = [12]                 # å¯è°ƒ 
# modes_list = [[10, 10, 10]]            # patch_size
# n_heads_list = [4]                  # å¯è°ƒ 
# lr_list = [0.001]                   # å¯è°ƒ

####################################################
####################################################

# æ„å»ºæ‰€æœ‰ç»„åˆï¼ˆå…± 3*2*3*3 = 54 ä¸ªä»»åŠ¡ï¼‰
grid = list(itertools.product(model_list, batch_size_list, modes_list, width_list, n_layers_list, n_heads_list, lr_list))

# æ„å»ºä»»åŠ¡é˜Ÿåˆ—
task_queue = Queue()
for model, batch_size, modes, width, n_layers, n_heads, lr in grid:
    cmd = (
        f"python {XDEBench_DIR}/train_xdebench.py "
        f"--data_path /aisi-nas/baixuan/XDEBench_Data/3dTGV128 "
        f"--experiment TGV "
        f"--model {model} "
        f"--batch_size {batch_size} "
        f"--modes {' '.join(map(str, modes))} "
        f"--width {width} "
        f"--n_layers {n_layers} "
        f"--n_heads {n_heads} "
        f"--lr {lr} "
        f"--num_iterations 20000 "
        f"--optim_type adam "
        f"--device cuda"
    )
    task_queue.put(cmd)

        # f"--data_path /aisi-nas/baixuan/XDEBench_Data/3dPoolfire "
        # f"--experiment poolfire "

        # f"--data_path /aisi-nas/baixuan/XDEBench_Data/3dTGV128 "
        # f"--experiment TGV "
        
        # f"--data_path /aisi-nas/baixuan/XDEBench_Data/3dTGV256 "
        # f"--experiment TGV "
        
        # f"--data_path /aisi-nas/baixuan/XDEBench_Data/3dHomoTurb "
        # f"--experiment homoturb "

        # f"--random_crop False "
        # f"--crop_size 128 "
        # f"--overlap_size 0 "
        # f"--n_patches 3 "    
    
# æ¯ä¸ª GPU è·‘ä¸€ä¸ªä»»åŠ¡ï¼Œä¸æ–­ä»é˜Ÿåˆ—ä¸­å–ä»»åŠ¡è¿è¡Œ
def gpu_worker(gpu_id, task_queue):
    while not task_queue.empty():
        try:
            cmd = task_queue.get_nowait()
        except:
            break
        env = os.environ.copy()
        env["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        print(f"ğŸš€ [GPU {gpu_id}] {cmd}")
        subprocess.run(cmd, shell=True, env=env)
        time.sleep(1)  # å¯é€‰ï¼šé¿å…çˆ†å‘å¼è°ƒåº¦
    print(f"âœ… GPU {gpu_id} å®Œæˆæ‰€æœ‰ä»»åŠ¡")

if __name__ == "__main__":
    print(f"Total jobs: {task_queue.qsize()} using {len(gpus)} GPUs")
    workers = []
    for gpu in gpus:
        p = Process(target=gpu_worker, args=(gpu, task_queue))
        p.start()
        workers.append(p)
    for p in workers:
        p.join()
    print("ğŸ‰ All tasks completed.")
