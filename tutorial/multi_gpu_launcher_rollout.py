import os
import itertools
import subprocess
import time
from multiprocessing import Process, Queue

XDEBench_DIR = f'../XDEBench'

# å¯ç”¨ GPU åˆ—è¡¨
gpus = [1]  # æ ¹æ®å®é™…æœºå™¨æƒ…å†µä¿®æ”¹

# =========================================
# Detonation Best
# =========================================
# model_list = ['FactFormerv2']     # gpu1 - 8h
# batch_size_list = [4]         # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [64]             # å¯è°ƒ
# n_layers_list = [3]            # å¯è°ƒ
# modes_list = [0]                  # æ— ç”¨å‚æ•°
# n_heads_list = [8]          # å¯è°ƒ    
# lr_list = [0.001]                 # å¯è°ƒ

# model_list = ['Transolver']       # gpu7 - 8h
# batch_size_list = [4]         # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [64]       # å¯è°ƒ
# n_layers_list = [4]            # å¯è°ƒ 
# modes_list = [0]                  # æ— ç”¨å‚æ•°
# n_heads_list = [6]             # å¯è°ƒ 
# lr_list = [0.0005]         # å¯è°ƒ

# model_list = ['DPOT']             # gpu3 - 0.7h
# batch_size_list = [7]        # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [512]     # å¯è°ƒ
# n_layers_list = [6]            # å¯è°ƒ
# modes_list = [64]             # ç¡®è®¤ modes <= img_size // 2
# n_heads_list = [0]                # æ— ç”¨å‚æ•°
# lr_list = [0.001]                 # å¯è°ƒ

# model_list = ['CROP']             # gpu6 - 2.5h
# batch_size_list = [7]     # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [192]           # å¯è°ƒ
# n_layers_list = [0]               # æ— ç”¨å‚æ•°
# modes_list = [32]         # å¯è°ƒ
# n_heads_list = [0]                # æ— ç”¨å‚æ•°
# lr_list = [0.001]                 # å¯è°ƒ   

# model_list = ['FFNO']             # gpu2 - 4h
# batch_size_list = [7]     # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [64]        # å¯è°ƒ
# n_layers_list = [3]            # å¯è°ƒ
# modes_list = [32]             # å¯è°ƒ
# n_heads_list = [0]                # æ— ç”¨å‚æ•°
# lr_list = [0.005]          # å¯è°ƒ

# model_list = ['GNOT']             # gpu5 - 4h
# batch_size_list = [1]          # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [64]            # å¯è°ƒ
# n_layers_list = [4]            # å¯è°ƒ
# modes_list = [0]                  # æ— ç”¨å‚æ•°
# n_heads_list = [4]             # ç¡®è®¤ width // heads èƒ½æ•´é™¤
# lr_list = [0.001]                 # å¯è°ƒ

model_list = ['CNextv2']   # gpu0 - 6.5h
batch_size_list = [7]      # ç¡®è®¤ batch_size <= num_train_traj
width_list = [96]       # ç¡®è®¤ width è‡³å°‘èƒ½è¢«2æ•´é™¤ n_layers+1 æ¬¡
n_layers_list = [6]         # ç¡®è®¤ img_size è‡³å°‘èƒ½è¢«2æ•´é™¤ n_layers æ¬¡
modes_list = [0]                  # æ— ç”¨å‚æ•°
n_heads_list = [0]                # æ— ç”¨å‚æ•°
lr_list = [0.001]           # å¯è°ƒ

# model_list = ['UNOv2']            # gpu7 - 8h
# batch_size_list = [2]          # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [64]            # å¯è°ƒ
# n_layers_list = [0]               # æ— ç”¨å‚æ•°
# modes_list = [32]             # å¯è°ƒ
# n_heads_list = [0]                # æ— ç”¨å‚æ•°
# lr_list = [0.005]          # å¯è°ƒ

# model_list = ['UNO']              # gpu4 - 8h
# batch_size_list = [7]     # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [64]            # å¯è°ƒ
# n_layers_list = [0]               # æ— ç”¨å‚æ•°
# modes_list = [32]             # å¯è°ƒ
# n_heads_list = [0]                # æ— ç”¨å‚æ•°
# lr_list = [0.005]         # å¯è°ƒ

# model_list = ['FNO']              # gpu4 - 1.5h
# batch_size_list = [4, 7]         # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [96]      # å¯è°ƒ
# n_layers_list = [0]               # æ— ç”¨å‚æ•°
# modes_list = [32, 16]         # ç¡®è®¤ modes <= img_size // 2
# n_heads_list = [0]                # æ— ç”¨å‚æ•°
# lr_list = [0.001]                 # å¯è°ƒ

# model_list = ['CNext']            # gpu3 - 10h
# batch_size_list = [4]         # ç¡®è®¤ batch_size <= num_train_traj
# n_layers_list = [6]         # ç¡®è®¤ img_size è‡³å°‘èƒ½è¢«2æ•´é™¤ n_layers æ¬¡
# width_list = [64]        # ç¡®è®¤ width è‡³å°‘èƒ½è¢«2æ•´é™¤ n_layers+1 æ¬¡
# modes_list = [0]                  # æ— ç”¨å‚æ•°
# n_heads_list = [0]                # æ— ç”¨å‚æ•°
# lr_list = [0.001]          # å¯è°ƒ

# model_list = ['FactFormer']       # gpu5 - 2h
# batch_size_list = [4]     # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [64]            # å¯è°ƒ
# n_layers_list = [4]            # å¯è°ƒ
# modes_list = [0]                  # æ— ç”¨å‚æ•°
# n_heads_list = [4]             # ç¡®è®¤ width // heads èƒ½æ•´é™¤
# lr_list = [0.001]          # å¯è°ƒ

# model_list = ['ONO']              # gpu5 - 3h
# batch_size_list = [4]          # ç¡®è®¤ batch_size <= num_train_traj
# width_list = [96]            # å¯è°ƒ
# n_layers_list = [3]            # å¯è°ƒ   
# modes_list = [0]                  # æ— ç”¨å‚æ•°
# n_heads_list = [8]             # å¯è°ƒ
# lr_list = [0.001]           # å¯è°ƒ

####################################################
####################################################

# æ„å»ºæ‰€æœ‰ç»„åˆï¼ˆå…± 3*2*3*3 = 54 ä¸ªä»»åŠ¡ï¼‰
grid = list(itertools.product(model_list, batch_size_list, modes_list, width_list, n_layers_list, n_heads_list, lr_list))

# æ„å»ºä»»åŠ¡é˜Ÿåˆ—
task_queue = Queue()
for model, batch_size, modes, width, n_layers, n_heads, lr in grid:
    cmd = (
        f"python {XDEBench_DIR}/train_xdebench_rollout.py "
        f"--data_path /aisi-nas/baixuan/XDEBench_Data/2dDetonation "
        f"--experiment detonation "   
        f"--model {model} "
        f"--batch_size {batch_size} "
        f"--modes {modes} "
        f"--width {width} "
        f"--n_layers {n_layers} "
        f"--n_heads {n_heads} "
        f"--lr {lr} "
        f"--num_iterations 20000 "
        f"--device cuda"
    )
    task_queue.put(cmd)

        # f"--data_path /aisi-nas/baixuan/XDEBench_Data/2dEvojet "
        # f"--experiment evojet "

        # f"--data_path /aisi-nas/baixuan/XDEBench_Data/2dDetonation "
        # f"--experiment detonation "

        # f"--data_path /aisi-nas/baixuan/XDEBench_Data/2dHIT "
        # f"--experiment hit " 

# æ¯ä¸ª GPU è·‘ä¸€ä¸ªä»»åŠ¡ï¼Œä¸æ–­ä»é˜Ÿåˆ—ä¸­å–ä»»åŠ¡è¿è¡Œ
def gpu_worker(gpu_id, task_queue):
    while not task_queue.empty():
        try:
            cmd = task_queue.get_nowait()
        except:
            break
        env = os.environ.copy()
        env["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        print(f"ğŸš€ [GPU {gpu_id}] {cmd}")
        subprocess.run(cmd, shell=True, env=env)
        time.sleep(1)  # å¯é€‰ï¼šé¿å…çˆ†å‘å¼è°ƒåº¦
    print(f"âœ… GPU {gpu_id} å®Œæˆæ‰€æœ‰ä»»åŠ¡")

if __name__ == "__main__":
    print(f"Total jobs: {task_queue.qsize()} using {len(gpus)} GPUs")
    workers = []
    for gpu in gpus:
        p = Process(target=gpu_worker, args=(gpu, task_queue))
        p.start()
        workers.append(p)
    for p in workers:
        p.join()
    print("ğŸ‰ All tasks completed.")
